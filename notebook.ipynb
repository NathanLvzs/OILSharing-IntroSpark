{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## SparkContext\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkContext\n",
      "\n",
      "# SparkContext instance\n",
      "sc = SparkContext('local[2]', 'pyspark')\n",
      "print sc\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<pyspark.context.SparkContext object at 0x7f3703b0c9d0>\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Creating RDDs\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create RDDs from collections\n",
      "data = range(1, 6)\n",
      "print data\n",
      "\n",
      "# no computation occurs here\n",
      "# because Spark just simply records how to create the RDD\n",
      "rangeRDD = sc.parallelize(data, 2)\n",
      "print rangeRDD\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3, 4, 5]\n",
        "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create RDDs from a file\n",
      "txtFile = sc.textFile('test.txt', 3)\n",
      "print txtFile\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MapPartitionsRDD[2] at textFile at NativeMethodAccessorImpl.java:-2\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Examples of some transformations and actions\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# map\n",
      "timesthreeRDD = rangeRDD.map(lambda x: x * 3)\n",
      "# collect is an action which triggers computation\n",
      "print timesthreeRDD.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[3, 6, 9, 12, 15]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# filter\n",
      "evenRDD = rangeRDD.filter(lambda x: x % 2 == 0)\n",
      "print evenRDD.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[2, 4]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# flatMap\n",
      "selfandtimestwoRDD = rangeRDD.flatMap(lambda x: [x, x*2])\n",
      "print selfandtimestwoRDD.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 2, 4, 3, 6, 4, 8, 5, 10]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# collect is shown above\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# take\n",
      "print rangeRDD.take(3)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# takeOrdered\n",
      "print rangeRDD.takeOrdered(3, lambda x: -1 * x)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[5, 4, 3]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Helloworld -- Wordcount\n",
      "\n",
      "sample data: `Big data is a buzzword nowadays, many companies use big data processing platforms to tackle big data problems. Apache Spark has quite some advantages compared to others, and is rapidly becoming the compute engine of choice for big data. This talk covers Apache Spark's architecture, programming model and how to write basic application with its commonly used APIs in Python. `"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from operator import add\n",
      "import re\n",
      "\n",
      "def tokenize(text):\n",
      "    # [^\\w] matches anything that's not alphanumeric or underscore\n",
      "    return re.sub(r'[^\\w]', ' ', text).lower().split()\n",
      "\n",
      "words = txtFile.flatMap(tokenize)\n",
      "\n",
      "wcpairs = words.map(lambda x: (x,1))\n",
      "\n",
      "counts = wcpairs.reduceByKey(add)\n",
      "# print counts\n",
      "\n",
      "# apply 'collect' action\n",
      "wcresult = counts.collect()\n",
      "# sort the result for output\n",
      "resultsorted = sorted(wcresult, key=(lambda (w,c): c), reverse=True)\n",
      "\n",
      "print resultsorted\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'big', 4), (u'data', 4), (u'to', 3), (u'spark', 2), (u'is', 2), (u'and', 2), (u'apache', 2), (u'buzzword', 1), (u'for', 1), (u'how', 1), (u'python', 1), (u'some', 1), (u'processing', 1), (u'problems', 1), (u'covers', 1), (u'choice', 1), (u'companies', 1), (u'used', 1), (u'advantages', 1), (u'tackle', 1), (u'has', 1), (u'with', 1), (u'talk', 1), (u'compared', 1), (u'rapidly', 1), (u'apis', 1), (u'this', 1), (u'basic', 1), (u'programming', 1), (u'its', 1), (u'architecture', 1), (u'many', 1), (u'model', 1), (u'becoming', 1), (u'engine', 1), (u'use', 1), (u'compute', 1), (u'of', 1), (u'in', 1), (u'commonly', 1), (u'quite', 1), (u'write', 1), (u'platforms', 1), (u's', 1), (u'application', 1), (u'a', 1), (u'others', 1), (u'the', 1), (u'nowadays', 1)]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Simple Community Detection\n",
      "\n",
      "With the popularity of online social networks, such as Facebook, Twitter and Sina Weibo, it is common requirement to find a way to detect commnunities among users. \n",
      "\n",
      "Basically, people belong to the same community when they have high similarity (i.e. sharing many common interests). There are two types of relationship in online social networks. One is symmetric, which means when Alice is Bob\u2019s friend, Bob will also be Alice\u2019s friend\u037e the other is asymmetric, which means Bob may not be Alice\u2019s friend even Alice is Bob\u2019s friend. In the second case, there are two roles in the relationship: follower and followee (like the case when using Twitter and Weibo). When Alice follows Bob, Alice is the follower and Bob is the followee.\n",
      "\n",
      "In order to detect communities, we need to calculate the similarity between any pair of users. Here, similarity is measured by **the number of common followees**. \n",
      "\n",
      "### Example Network\n",
      "\n",
      "The following is the figure of a example user network.\n",
      "\n",
      "<img src=\"files/graph.png\" alt=\"User Graph\" style=\"width: 350px;\"/>\n",
      "<!--![User Graph](files/graph.png)-->\n",
      "\n",
      "Corresponding data in the format **followee: follower#1 follower#2 follower#3 ...**\n",
      "```\n",
      "1:   2,4  \n",
      "2:   1  \n",
      "3:   1,2,5  \n",
      "5:   1,2,3  \n",
      "```\n",
      "\n",
      "### Programming Procedure\n",
      "\n",
      "For a user (A)\n",
      "1. Find all persons who has followed the same person with A\n",
      "2. Calculate their similarity with A\n",
      "3. Sort by similarity, get the top K persons\n",
      "\n",
      "Put the above procedure into RDD operation flow, \n",
      "\n",
      "<img src=\"files/dataflow.png\" alt=\"data flow\" style=\"width: 900px;\"/>\n",
      "\n",
      "It is quite similar to wordcount after applying the groupByKey the transformation. And in the end, sort to get the result. \n",
      "\n",
      "*Just for illustration, not quite efficient, still room for improvement*\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "import operator\n",
      "\n",
      "rawFile = sc.textFile('users.txt')\n",
      "\n",
      "def makePair(line):\n",
      "    strFollowers = line.split(':')[1]\n",
      "    followers = map(int, strFollowers.split(','))\n",
      "    count = len(followers)\n",
      "    pairs = []\n",
      "    if count > 1:\n",
      "        for index_i in range(count):\n",
      "            for index_j in range(index_i+1, count):\n",
      "                pairs.append((followers[index_i], followers[index_j]))\n",
      "                pairs.append((followers[index_j], followers[index_i]))\n",
      "    return pairs\n",
      "\n",
      "def makeDict(record):\n",
      "    \"\"\"\n",
      "    record: (active user, [users who share a same followee with the active user])\n",
      "    \"\"\"\n",
      "    result = defaultdict(int)\n",
      "    for user in record[1]:\n",
      "        result[user] += 1\n",
      "    return (record[0], sorted(result.iteritems(), key=operator.itemgetter(1), reverse=True))\n",
      "\n",
      "pairsRDD = rawFile.flatMap(makePair)\n",
      "print 'pairs: ', pairsRDD.collect()\n",
      "\n",
      "groupRDD = pairsRDD.groupByKey()\n",
      "# print groupRDD.collect()\n",
      "\n",
      "dictRDD = groupRDD.map(makeDict)\n",
      "print dictRDD.collect()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pairs:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(2, 4), (4, 2), (1, 2), (2, 1), (1, 5), (5, 1), (2, 5), (5, 2), (1, 2), (2, 1), (1, 3), (3, 1), (2, 3), (3, 2)]\n",
        "[(2, [(1, 2), (3, 1), (4, 1), (5, 1)]), (4, [(2, 1)]), (1, [(2, 2), (3, 1), (5, 1)]), (3, [(1, 1), (2, 1)]), (5, [(1, 1), (2, 1)])]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}